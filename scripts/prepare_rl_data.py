import json
import os
import random

# ç¡®ä¿éšæœºæ€§å¯æ§
random.seed(42)

INPUT_FILES = {
    "gold": "data/processed/gold_set.jsonl",
    "red": "data/processed/red_team.jsonl",
    "safety_red": "data/rl/safety_red_team.jsonl",  # æ–°å¢ï¼šåˆè§„å®‰å…¨çº¢é˜Ÿæ•°æ®
    "train": "data/processed/train.jsonl"
}
OUTPUT_FILE = "data/rl/training_prompts.jsonl"
TARGET_SIZE = 2000

# é’ˆå¯¹åˆè§„å®‰å…¨ç›®æ ‡çš„é‡‡æ ·ç­–ç•¥
SAFETY_FOCUS_SAMPLING = {
    "safety_red_oversampling": 3,  # å®‰å…¨çº¢é˜Ÿæ ·æœ¬é‡å¤3æ¬¡ï¼ˆå¼ºåŒ–è®­ç»ƒï¼‰
    "high_risk_ratio": 0.6,        # 60%é«˜é£é™©æ ·æœ¬
    "general_ratio": 0.4           # 40%ä¸€èˆ¬æ ·æœ¬
}

def load_jsonl(path):
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f]

def main():
    print("ğŸ”„ Loading source datasets...")
    gold = load_jsonl(INPUT_FILES["gold"])
    red = load_jsonl(INPUT_FILES["red"])
    safety_red = load_jsonl(INPUT_FILES["safety_red"])
    train = load_jsonl(INPUT_FILES["train"])
    
    print(f"   Gold: {len(gold)}, Red: {len(red)}, Safety-Red: {len(safety_red)}, Train: {len(train)}")
    
    # æ–°ç­–ç•¥ï¼ˆé’ˆå¯¹åˆè§„å®‰å…¨ç›®æ ‡ï¼‰ï¼š
    # 1. å®‰å…¨çº¢é˜Ÿæ ·æœ¬ Ã— 3 (è¿‡é‡‡æ ·ï¼Œå¼ºåŒ–å®‰å…¨æ€§å­¦ä¹ )
    # 2. ä¸€èˆ¬çº¢é˜Ÿæ ·æœ¬ (åŸæœ‰é«˜é£é™©åœºæ™¯)
    # 3. Gold Set æ ·æœ¬ (é«˜è´¨é‡é”šç‚¹)
    # 4. ä» Train ä¸­æŒ‰é£é™©ç­‰çº§é‡‡æ ·ï¼š60%é«˜é£é™© + 40%ä¸€èˆ¬
    
    dataset = []
    
    # å®‰å…¨çº¢é˜Ÿæ ·æœ¬è¿‡é‡‡æ ·ï¼ˆé‡å¤3æ¬¡å¼ºåŒ–è®­ç»ƒï¼‰
    oversampling_times = SAFETY_FOCUS_SAMPLING["safety_red_oversampling"]
    for _ in range(oversampling_times):
        dataset.extend(safety_red)
    
    # æ·»åŠ å…¶ä»–é«˜ä¼˜å…ˆçº§æ ·æœ¬
    dataset.extend(red)
    dataset.extend(gold)
    
    # å»é‡ (ä»¥ input ä¸º key)
    seen = set()
    unique_dataset = []
    for item in dataset:
        if item["input"] not in seen:
            unique_dataset.append(item)
            seen.add(item["input"])
            
    current_count = len(unique_dataset)
    needed = max(0, TARGET_SIZE - current_count)
    
    if needed > 0 and len(train) > 0:
        # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„
        train_filtered = [x for x in train if x["input"] not in seen]
        
        # æŒ‰é£é™©ç­‰çº§åˆ†ç»„
        high_risk_train = [x for x in train_filtered 
                          if x.get("meta", {}).get("risk_level") in ["high", "critical"]]
        general_train = [x for x in train_filtered 
                        if x.get("meta", {}).get("risk_level") not in ["high", "critical"]]
        
        # è®¡ç®—é‡‡æ ·æ•°é‡
        high_risk_count = int(needed * SAFETY_FOCUS_SAMPLING["high_risk_ratio"])
        general_count = needed - high_risk_count
        
        # é‡‡æ ·
        sampled_high = random.sample(high_risk_train, min(len(high_risk_train), high_risk_count))
        sampled_general = random.sample(general_train, min(len(general_train), general_count))
        
        unique_dataset.extend(sampled_high)
        unique_dataset.extend(sampled_general)
        
        print(f"   ğŸ“Š Sampling from train: {len(sampled_high)} high-risk + {len(sampled_general)} general")
        
    # ç®€åŒ–å­—æ®µï¼ŒRL åªéœ€è¦ prompt (input)
    # ä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä¿ç•™åŸå§‹ç»“æ„
    
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for item in unique_dataset:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    # ç»Ÿè®¡æœ€ç»ˆæ„æˆ
    risk_stats = {}
    for item in unique_dataset:
        risk = item.get("meta", {}).get("risk_level", "unknown")
        risk_stats[risk] = risk_stats.get(risk, 0) + 1
            
    print(f"\nâœ… Generated {len(unique_dataset)} RL prompts at {OUTPUT_FILE}")
    print(f"   ğŸ“Š Risk Level Distribution:")
    for risk, count in sorted(risk_stats.items()):
        print(f"      {risk}: {count} ({count/len(unique_dataset)*100:.1f}%)")
    print(f"   ğŸ¯ Strategy: Safety-focused (60% high-risk + oversampled safety cases)")

if __name__ == "__main__":
    main()
