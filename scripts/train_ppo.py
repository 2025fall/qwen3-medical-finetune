import os
import torch
from dataclasses import dataclass, field
from typing import Optional
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, Adafactor

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from reward_fn import RewardEngine

# ================= é…ç½® =================
@dataclass
class ScriptArguments:
    model_name: str = field(default="models/Qwen/Qwen3-1.7B", metadata={"help": "Base model path"})
    adapter_path: str = field(default="models/lora/final_lora", metadata={"help": "SFT LoRA path"})
    log_with: Optional[str] = field(default=None, metadata={"help": "use 'wandb' to log"})
    learning_rate: float = field(default=1.41e-5, metadata={"help": "the learning rate"})
    batch_size: int = field(default=4, metadata={"help": "the batch size"})
    mini_batch_size: int = field(default=1, metadata={"help": "the PPO mini batch size"})
    gradient_accumulation_steps: int = field(default=1, metadata={"help": "the number of gradient accumulation steps"})
    output_dir: str = field(default="models/rl/checkpoints", metadata={"help": "Output directory"})

def main():
    parser = PPOConfig(
        model_name="qwen3-medical-rl",
        learning_rate=1.41e-5,
        batch_size=4,
        mini_batch_size=1,
        gradient_accumulation_steps=1,
        optimize_cuda_cache=True,
        target_kl=0.1,
        ppo_epochs=4,
        seed=42,
    )
    # è¿™é‡Œç®€åŒ–å‚æ•°è§£æï¼Œå®é™…å¯ç”¨ HfArgumentParser
    config = parser

    # 1. åˆå§‹åŒ–æ¨¡å‹ä¸ Tokenizer
    # Load the base model and attach the value head
    # æ³¨æ„ï¼šé€šå¸¸æˆ‘ä»¬éœ€è¦å…ˆåŠ è½½ SFT åçš„æ¨¡å‹ã€‚
    # å¦‚æœ adapter_path å­˜åœ¨ï¼Œæˆ‘ä»¬åº”è¯¥åŠ è½½ base + adapterï¼Œç„¶åè½¬ä¸º AutoModelForCausalLMWithValueHead
    
    base_model_path = "models/Qwen/Qwen3-1.7B" # è¯·ç¡®ä¿æ­¤è·¯å¾„æ­£ç¡®ï¼Œæˆ–ä»å‚æ•°ä¼ å…¥
    sft_adapter_path = "models/lora/final_lora"
    
    print(f"Loading model from {base_model_path} and adapter {sft_adapter_path}...")
    
    # TRL çš„è¿™ä¸ªç±»ä¼šè‡ªåŠ¨å¤„ç† PEFT
    # ä½†æˆ‘ä»¬éœ€è¦ç¡®ä¿å®ƒåŠ è½½äº†æˆ‘ä»¬çš„ SFT adapter
    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        base_model_path,
        peft_config=LoraConfig.from_pretrained(sft_adapter_path) if os.path.exists(sft_adapter_path) else None,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # å¦‚æœ peft_config æ²¡ç”Ÿæ•ˆï¼ˆä¾‹å¦‚ AutoModel... ä¸ç›´æ¥æ”¯æŒä»ç›®å½•åŠ è½½ adapterï¼‰ï¼Œ
    # å¯èƒ½éœ€è¦æ‰‹åŠ¨ mergeï¼Œä½†åœ¨ TRL ä¸­é€šå¸¸æ¨èç›´æ¥ç”¨ peft_config é‡æ–°åˆå§‹åŒ– LoRAï¼Œ
    # æˆ–è€…è®© model å·²ç»æ˜¯ä¸€ä¸ª peft modelã€‚
    # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾è¿™é‡Œæ˜¯åœ¨ SFT åŸºç¡€ä¸Šç»§ç»­å¾®è°ƒï¼Œæ‰€ä»¥è®© Value Head ä¹Ÿæ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œ
    # è€Œ Policy ç»§æ‰¿äº† SFT çš„æƒé‡ã€‚
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    # 2. å‡†å¤‡æ•°æ®
    # ä¼˜å…ˆå¯»æ‰¾ RL ä¸“ç”¨æç¤ºï¼Œå¦åˆ™ç”¨ SFT æ•°æ®
    data_path = "data/rl/training_prompts.jsonl"
    if not os.path.exists(data_path):
        print(f"âš ï¸ {data_path} not found, falling back to data/processed/train.jsonl")
        data_path = "data/processed/train.jsonl"
        
    dataset = load_dataset("json", data_files=data_path, split="train")
    
    def tokenize(sample):
        # æ„å»º Prompt
        # æ ¼å¼ï¼š <|im_start|>system\n...<|im_end|>\n<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n
        # è¿™é‡Œæˆ‘ä»¬åªè´Ÿè´£æŠŠ query å˜æˆ input_ids
        # å‡è®¾ tokenizer.apply_chat_template å¯ç”¨
        # ä½† PPO generate éœ€è¦çº¯ tensor
        
        prompt_text = f"ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚\nUser: {sample['input']}\nAssistant:"
        # ç®€å•æ‹¼æ¥ï¼Œé¿å… template å¤æ‚æ€§
        
        sample["input_ids"] = tokenizer.encode(prompt_text, return_tensors="pt")[0]
        sample["query"] = sample["input"] # ç”¨äº Reward Function
        return sample

    dataset = dataset.map(tokenize, batched=False)
    dataset.set_format(type="torch")

    # 3. åˆå§‹åŒ– Trainer
    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = Adafactor(
        filter(lambda p: p.requires_grad, model.parameters()),
        scale_parameter=False,
        relative_step=False,
        warmup_init=False,
        lr=config.learning_rate,
    )

    ppo_trainer = PPOTrainer(
        config,
        model,
        ref_model=None, # TRL ä¼šè‡ªåŠ¨å¤åˆ¶ä¸€ä»½ä½œä¸º ref_model
        tokenizer=tokenizer,
        dataset=dataset,
        data_collator=None, # TRL é»˜è®¤
        optimizer=optimizer,
    )

    # 4. åˆå§‹åŒ–å¥–åŠ±å¼•æ“
    reward_engine = RewardEngine()

    # 5. è®­ç»ƒå¾ªç¯
    generation_kwargs = {
        "min_length": -1,
        "top_k": 0.0,
        "top_p": 1.0,
        "do_sample": True,
        "pad_token_id": tokenizer.eos_token_id,
        "max_new_tokens": 256,
    }

    print("ğŸš€ Starting PPO training...")
    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
        query_tensors = batch["input_ids"]
    
        # Get response from Policy
        response_tensors = ppo_trainer.generate(
            query_tensors, return_prompt=False, **generation_kwargs
        )
        
        batch["response"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)
        batch["query"] = tokenizer.batch_decode(query_tensors, skip_special_tokens=True) # Decode for teacher

        # Compute Rewards
        # æ³¨æ„ï¼šreward_engine éœ€è¦çº¯æ–‡æœ¬çš„ query å’Œ response
        # è¿™é‡Œ batch["query"] å¯èƒ½åŒ…å« system promptï¼Œteacher éœ€è¦çº¯é—®é¢˜å—ï¼Ÿ
        # æ˜¯çš„ï¼Œteacher éœ€è¦çº¯é—®é¢˜ã€‚æˆ‘ä»¬åœ¨ dataset æ„å»ºæ—¶ä¿ç•™äº†åŸå§‹ inputã€‚
        # ä½† dataloader å‡ºæ¥çš„ batch åªæœ‰ tensorsï¼Œé™¤éæˆ‘ä»¬è‡ªå®šä¹‰ collatorã€‚
        # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä» decoded query ä¸­æå– User çš„é—®é¢˜ï¼Œæˆ–è€…å¦‚æœ batch ä¸­ä¿ç•™äº† raw text (PPOTrainer ä¸ä¸€å®šä¿ç•™)ã€‚
        # ä¿®æ­£ï¼šæˆ‘ä»¬éœ€è¦åœ¨ tokenize æ—¶ä¸æŠŠ query ä¸¢æ‰ï¼Œæˆ–è€…é‡æ–°è§£æã€‚
        # ä¸ºäº†ç¨³å¥ï¼Œæˆ‘ä»¬å°è¯•ä» decoded query æå–é—®é¢˜ã€‚
        
        prompts_for_reward = [q.split("User: ")[-1].split("\nAssistant")[0] for q in batch["query"]]
        rewards = reward_engine.compute_rewards(prompts_for_reward, batch["response"])

        # Run PPO step
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        
        # Log
        ppo_trainer.log_stats(stats, batch, rewards)
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Mean Reward = {torch.stack(rewards).mean().item():.2f}")
            
        # Save periodically
        if epoch > 0 and epoch % 50 == 0:
            ppo_trainer.save_pretrained(os.path.join(config.output_dir, f"step_{epoch}"))

    # Save final
    ppo_trainer.save_pretrained(os.path.join(config.output_dir, "final_rl_model"))
    print("âœ… Training finished. Model saved.")

if __name__ == "__main__":
    main()
