import os
import torch
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
import transformers

# å…¼å®¹ TRL 0.7.x å¯¹ transformers.top_k_top_p_filtering çš„ä¾èµ–
try:
    from transformers.generation.utils import top_k_top_p_filtering  # æ—§ç‰ˆå…¥å£
except Exception:
    try:
        from transformers.generation.logits_process import top_k_top_p_filtering  # æ›´æ—§ç‰ˆå…¥å£
    except Exception:
        # Fallback: ç®€å•å®ç°ä¸€ä¸ª top-k / top-p è¿‡æ»¤å‡½æ•°
        def top_k_top_p_filtering(
            logits: torch.Tensor,
            top_k: int = 0,
            top_p: float = 1.0,
            filter_value: float = -float("inf"),
            min_tokens_to_keep: int = 1,
        ) -> torch.Tensor:
            """è½»é‡ç‰ˆ top-k/top-p è¿‡æ»¤ï¼Œä¾› TRL ä¾èµ–è°ƒç”¨ã€‚"""
            if top_k > 0:
                top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))
                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
                logits = logits.masked_fill(indices_to_remove, filter_value)

            if 0 < top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                # ä¿ç•™è‡³å°‘ min_tokens_to_keep
                sorted_indices_to_remove[..., :min_tokens_to_keep] = 0
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                logits = logits.masked_fill(indices_to_remove, filter_value)

            return logits

# å°†å‡½æ•°æŒ‚åˆ° transformers å‘½åç©ºé—´ï¼Œä¾› TRL import
setattr(transformers, "top_k_top_p_filtering", top_k_top_p_filtering)
from transformers import AutoTokenizer, Adafactor, DataCollatorWithPadding

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from reward_fn import RewardEngine

# ================= é…ç½® =================
@dataclass
class ScriptArguments:
    model_name: str = field(default="models/Qwen/Qwen3-1.7B", metadata={"help": "Base model path"})
    adapter_path: str = field(default="models/lora/final_lora", metadata={"help": "SFT LoRA path"})
    log_with: Optional[str] = field(default=None, metadata={"help": "use 'wandb' to log"})
    learning_rate: float = field(default=1.41e-5, metadata={"help": "the learning rate"})
    batch_size: int = field(default=4, metadata={"help": "the batch size"})
    mini_batch_size: int = field(default=1, metadata={"help": "the PPO mini batch size"})
    gradient_accumulation_steps: int = field(default=1, metadata={"help": "the number of gradient accumulation steps"})
    output_dir: str = field(default="models/rl/checkpoints", metadata={"help": "Output directory"})

def main():
    parser = PPOConfig(
        model_name="qwen3-medical-rl",
        learning_rate=1.41e-5,
        batch_size=4,
        mini_batch_size=1,
        gradient_accumulation_steps=1,
        optimize_cuda_cache=True,
        target_kl=0.1,
        ppo_epochs=4,
        seed=42,
    )
    # è¿™é‡Œç®€åŒ–å‚æ•°è§£æï¼Œå®é™…å¯ç”¨ HfArgumentParser
    config = parser

    # 1. åˆå§‹åŒ–æ¨¡å‹ä¸ Tokenizer
    base_model_path = os.environ.get("BASE_MODEL_PATH", "models/Qwen/Qwen3-1.7B")  # ç¯å¢ƒå˜é‡å¯è¦†ç›–
    sft_adapter_path = os.environ.get("LORA_ADAPTER_PATH", "models/lora/final_lora")
    
    print(f"Loading model from {base_model_path} and adapter {sft_adapter_path}...")
    
    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        base_model_path,
        peft_config=LoraConfig.from_pretrained(sft_adapter_path) if os.path.exists(sft_adapter_path) else None,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # å¦‚æœ peft_config æ²¡ç”Ÿæ•ˆï¼ˆä¾‹å¦‚ AutoModel... ä¸ç›´æ¥æ”¯æŒä»ç›®å½•åŠ è½½ adapterï¼‰ï¼Œ
    # å¯èƒ½éœ€è¦æ‰‹åŠ¨ mergeï¼Œä½†åœ¨ TRL ä¸­é€šå¸¸æ¨èç›´æ¥ç”¨ peft_config é‡æ–°åˆå§‹åŒ– LoRAï¼Œ
    # æˆ–è€…è®© model å·²ç»æ˜¯ä¸€ä¸ª peft modelã€‚
    # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾è¿™é‡Œæ˜¯åœ¨ SFT åŸºç¡€ä¸Šç»§ç»­å¾®è°ƒï¼Œæ‰€ä»¥è®© Value Head ä¹Ÿæ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œ
    # è€Œ Policy ç»§æ‰¿äº† SFT çš„æƒé‡ã€‚
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    # 2. å‡†å¤‡æ•°æ®ï¼ˆè‹¥ç¼ºå¤±åˆ™ç”Ÿæˆä¸€ä¸ªå°å‹å®‰å…¨ RL æ•°æ®é›†ï¼‰
    data_path = "data/rl/training_prompts.jsonl"
    os.makedirs(os.path.dirname(data_path), exist_ok=True)
    if not os.path.exists(data_path):
        print(f"âš ï¸ {data_path} not found, auto-generating a small RL dataset.")
        seed_data = [
            {
                "instruction": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                "input": "æˆ‘çˆ¸çªç„¶å‰§çƒˆèƒ¸ç—›å‡ºå†·æ±—ï¼Œè¿˜å‘¼å¸å›°éš¾ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ",
                "output": "<think>è¯„ä¼°æ€¥æ€§å† è„‰ç»¼åˆå¾/ä¸»åŠ¨è„‰å¤¹å±‚/è‚ºæ “å¡çº¢æ——...</think> è¯·ç«‹å³æ‹¨æ‰“120å¹¶ä¿æŒå®‰é™ä½“ä½ï¼Œç­‰å¾…æ€¥æ•‘ã€‚",
            },
            {
                "instruction": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                "input": "ä¸¤å²å­©å­æŒç»­é«˜çƒ§40åº¦ï¼Œè¿˜æŠ½æäº†æ€ä¹ˆåŠï¼Ÿ",
                "output": "<think>å„¿ç«¥é«˜çƒ­æƒŠå¥ï¼Œé«˜é£é™©ï¼›éœ€ç´§æ€¥å°±åŒ»ã€‚</think> ä¾§å§ä½é˜²è¯¯å¸ï¼Œç«‹å³æ€¥è¯Šæˆ–æ‹¨æ‰“120ã€‚",
            },
            {
                "instruction": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                "input": "å¤´ç—›ä¸‰å¤©äº†ï¼Œå¯ä»¥åƒé˜¿è«è¥¿æ—å—ï¼Ÿ",
                "output": "<think>å¤´ç—›ç—…å› å¤æ‚ï¼ŒæŠ—ç”Ÿç´ æ»¥ç”¨é£é™©ã€‚</think> ä¸å»ºè®®è‡ªè¡Œç”¨é˜¿è«è¥¿æ—ï¼Œè‹¥æŒç»­æˆ–æœ‰çº¢æ——ç—‡çŠ¶è¯·å°±åŒ»ã€‚",
            },
            {
                "instruction": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                "input": "æˆ‘æ­£åœ¨æ€€å­•ï¼Œèƒ½ä¸èƒ½åƒå¸ƒæ´›èŠ¬ï¼Ÿ",
                "output": "<think>å­•æœŸè¯ç‰©éœ€è°¨æ…ã€‚</think> å¸ƒæ´›èŠ¬å­•æ—©æ™šæœŸéœ€é¿å…ï¼Œå»ºè®®å’¨è¯¢äº§ç§‘åŒ»ç”Ÿæˆ–ç”¨å¯¹ä¹™é…°æ°¨åŸºé…šã€‚",
            },
            {
                "instruction": "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚",
                "input": "æ„Ÿè§‰å¿ƒæ…Œèƒ¸é—·ï¼Œéœ€è¦å»åŒ»é™¢å—ï¼Ÿ",
                "output": "<think>éœ€æ’é™¤å¿ƒè¡€ç®¡é£é™©ã€‚</think> è‹¥ä¼´èƒ¸ç—›/å‡ºæ±—/å‘¼å¸å›°éš¾è¯·ç«‹å³å°±åŒ»ï¼Œå¿…è¦æ—¶æ‹¨æ‰“120ã€‚",
            },
        ]
        with open(data_path, "w", encoding="utf-8") as f:
            for it in seed_data:
                json.dump(it, f, ensure_ascii=False)
                f.write("\n")

    dataset = load_dataset("json", data_files=data_path, split="train")
    
    def tokenize(sample):
        prompt_text = (
            f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚<|im_end|>\n"
            f"<|im_start|>user\n{sample['input']}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
        tokenized = tokenizer(prompt_text, add_special_tokens=False)
        return {
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "query_text": sample["input"],  # åŸå§‹ç”¨æˆ·é—®é¢˜ï¼Œç”¨äºå¥–åŠ±
        }

    dataset = dataset.map(tokenize, batched=False, remove_columns=dataset.column_names)

    def collate_fn(features: List[Dict[str, Any]]) -> Dict[str, Any]:
        # å°†åˆ—è¡¨å½¢å¼çš„ input_ids/attention_mask pad æˆå¼ é‡ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹ query_text
        batch = tokenizer.pad(
            {k: [f[k] for f in features] for k in ["input_ids", "attention_mask"]},
            padding=True,
            return_tensors="pt",
        )
        batch["query_text"] = [f["query_text"] for f in features]
        return batch

    # 3. åˆå§‹åŒ– Trainer
    optimizer = Adafactor(
        filter(lambda p: p.requires_grad, model.parameters()),
        scale_parameter=False,
        relative_step=False,
        warmup_init=False,
        lr=config.learning_rate,
    )

    ppo_trainer = PPOTrainer(
        config,
        model,
        ref_model=None, # TRL ä¼šè‡ªåŠ¨å¤åˆ¶ä¸€ä»½ä½œä¸º ref_model
        tokenizer=tokenizer,
        dataset=dataset,
        data_collator=collate_fn,  # è‡ªå®šä¹‰ paddingï¼Œä¿ç•™ query_text
        optimizer=optimizer,
    )

    # 4. åˆå§‹åŒ–å¥–åŠ±å¼•æ“
    reward_engine = RewardEngine()

    # 5. è®­ç»ƒå¾ªç¯
    generation_kwargs = {
        "min_length": -1,
        "top_k": 0.0,
        "top_p": 1.0,
        "do_sample": True,
        "pad_token_id": tokenizer.eos_token_id,
        "max_new_tokens": 256,
    }

    print("ğŸš€ Starting PPO training...")
    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
        query_tensors = batch["input_ids"]
    
        # Get response from Policy
        response_tensors = ppo_trainer.generate(
            query_tensors, return_prompt=False, **generation_kwargs
        )
        
        batch["response"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)
        prompts_for_reward = batch["query_text"]
        rewards = reward_engine.compute_rewards(prompts_for_reward, batch["response"])

        # Run PPO step
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        
        # Log
        ppo_trainer.log_stats(stats, batch, rewards)
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Mean Reward = {torch.stack(rewards).mean().item():.2f}")
            
        # Save periodically
        if epoch > 0 and epoch % 50 == 0:
            ppo_trainer.save_pretrained(os.path.join(config.output_dir, f"step_{epoch}"))

    # Save final
    ppo_trainer.save_pretrained(os.path.join(config.output_dir, "final_rl_model"))
    print("âœ… Training finished. Model saved.")

if __name__ == "__main__":
    main()
