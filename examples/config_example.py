#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é…ç½®æ–‡ä»¶ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•è‡ªå®šä¹‰è®­ç»ƒå’Œæ¨ç†å‚æ•°
"""

# ===== è®­ç»ƒé…ç½® =====

class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    
    # åŸºç¡€é…ç½®
    MODEL_NAME = "Qwen/Qwen3-1.7B"
    DATA_DIR = "data/processed"
    OUTPUT_DIR = "models"
    
    # è®­ç»ƒå‚æ•°
    LEARNING_RATE = 2e-4  # LoRAæ¨èå€¼ï¼Œå…¨å‚æ•°å¾®è°ƒå»ºè®®5e-6
    NUM_EPOCHS = 3
    BATCH_SIZE = 2
    GRADIENT_ACCUMULATION_STEPS = 2
    
    # LoRAé…ç½®
    LORA_R = 64
    LORA_ALPHA = 128
    LORA_DROPOUT = 0.1
    LORA_TARGET_MODULES = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ]
    
    # ä¼˜åŒ–å™¨é…ç½®
    WEIGHT_DECAY = 0.01
    WARMUP_RATIO = 0.1
    LR_SCHEDULER_TYPE = "cosine"
    
    # ä¿å­˜é…ç½®
    SAVE_STEPS = 400
    EVAL_STEPS = 100
    LOGGING_STEPS = 10

# ===== æ¨ç†é…ç½® =====

class InferenceConfig:
    """æ¨ç†é…ç½®ç±»"""
    
    # ç”Ÿæˆå‚æ•°
    MAX_NEW_TOKENS = 512
    TEMPERATURE = 0.7
    TOP_P = 0.9
    TOP_K = 50
    REPETITION_PENALTY = 1.1
    
    # ç³»ç»Ÿæç¤ºè¯
    SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚"
    
    # æ¨¡å‹è·¯å¾„
    MODEL_PATHS = {
        "lora": {
            "base_dir": "models/Qwen/Qwen3-1.7B",
            "adapter_dir": "models/lora/final_lora"
        },
        "full": {
            "model_dir": "models/full/final_model"
        }
    }

# ===== æ•°æ®å¤„ç†é…ç½® =====

class DataConfig:
    """æ•°æ®å¤„ç†é…ç½®ç±»"""
    
    # æ•°æ®è·¯å¾„
    RAW_DATA_DIR = "data/raw"
    PROCESSED_DATA_DIR = "data/processed"
    
    # æ•°æ®åˆ†å‰²æ¯”ä¾‹
    TRAIN_RATIO = 0.8
    DEV_RATIO = 0.1
    TEST_RATIO = 0.1
    
    # æ–‡æœ¬å¤„ç†
    MAX_LENGTH = 2048
    MIN_LENGTH = 10
    
    # å»é‡é…ç½®
    DEDUP_BY_QUESTION = True
    DEDUP_BY_SEMANTIC = True

# ===== è¯„ä¼°é…ç½® =====

class EvalConfig:
    """è¯„ä¼°é…ç½®ç±»"""
    
    # è¯„ä¼°æŒ‡æ ‡
    METRICS = [
        "think_coverage",      # æ€è€ƒé“¾è¦†ç›–ç‡
        "urgent_coverage",     # ç´§æ€¥ä¿¡å·è¦†ç›–ç‡
        "risky_prescription_rate"  # é£é™©å¤„æ–¹ç‡
    ]
    
    # è¯„ä¼°æ•°æ®é›†
    EVAL_DATASETS = [
        "dev.jsonl",
        "test.jsonl", 
        "gold_set.jsonl",
        "red_team.jsonl"
    ]
    
    # è¾“å‡ºé…ç½®
    REPORT_DIR = "eval_report"
    SAVE_DETAILED_RESULTS = True

# ===== ä½¿ç”¨ç¤ºä¾‹ =====

def get_training_args():
    """è·å–è®­ç»ƒå‚æ•°"""
    config = TrainingConfig()
    
    return {
        "learning_rate": config.LEARNING_RATE,
        "num_train_epochs": config.NUM_EPOCHS,
        "per_device_train_batch_size": config.BATCH_SIZE,
        "gradient_accumulation_steps": config.GRADIENT_ACCUMULATION_STEPS,
        "warmup_ratio": config.WARMUP_RATIO,
        "lr_scheduler_type": config.LR_SCHEDULER_TYPE,
        "weight_decay": config.WEIGHT_DECAY,
        "save_steps": config.SAVE_STEPS,
        "eval_steps": config.EVAL_STEPS,
        "logging_steps": config.LOGGING_STEPS,
    }

def get_lora_config():
    """è·å–LoRAé…ç½®"""
    config = TrainingConfig()
    
    return {
        "r": config.LORA_R,
        "lora_alpha": config.LORA_ALPHA,
        "lora_dropout": config.LORA_DROPOUT,
        "target_modules": config.LORA_TARGET_MODULES,
    }

def get_inference_params():
    """è·å–æ¨ç†å‚æ•°"""
    config = InferenceConfig()
    
    return {
        "max_new_tokens": config.MAX_NEW_TOKENS,
        "temperature": config.TEMPERATURE,
        "top_p": config.TOP_P,
        "top_k": config.TOP_K,
        "repetition_penalty": config.REPETITION_PENALTY,
    }

# ===== è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹ =====

def create_custom_config():
    """åˆ›å»ºè‡ªå®šä¹‰é…ç½®ç¤ºä¾‹"""
    
    # å¿«é€Ÿè®­ç»ƒé…ç½®ï¼ˆé€‚åˆè°ƒè¯•ï¼‰
    quick_config = TrainingConfig()
    quick_config.NUM_EPOCHS = 1
    quick_config.BATCH_SIZE = 1
    quick_config.GRADIENT_ACCUMULATION_STEPS = 1
    quick_config.SAVE_STEPS = 50
    quick_config.EVAL_STEPS = 25
    
    # é«˜è´¨é‡è®­ç»ƒé…ç½®ï¼ˆé€‚åˆç”Ÿäº§ï¼‰
    quality_config = TrainingConfig()
    quality_config.NUM_EPOCHS = 5
    quality_config.BATCH_SIZE = 4
    quality_config.GRADIENT_ACCUMULATION_STEPS = 4
    quality_config.LEARNING_RATE = 1e-4
    
    # ä¿å®ˆæ¨ç†é…ç½®ï¼ˆé€‚åˆåŒ»ç–—åœºæ™¯ï¼‰
    conservative_inference = InferenceConfig()
    conservative_inference.TEMPERATURE = 0.3
    conservative_inference.TOP_P = 0.8
    conservative_inference.REPETITION_PENALTY = 1.2
    
    return {
        "quick": quick_config,
        "quality": quality_config,
        "conservative": conservative_inference
    }

if __name__ == "__main__":
    print("ğŸ“‹ é…ç½®æ–‡ä»¶ç¤ºä¾‹")
    print("=" * 30)
    
    print("è®­ç»ƒå‚æ•°:")
    print(get_training_args())
    
    print("\nLoRAé…ç½®:")
    print(get_lora_config())
    
    print("\næ¨ç†å‚æ•°:")
    print(get_inference_params())
    
    print("\nè‡ªå®šä¹‰é…ç½®:")
    custom_configs = create_custom_config()
    for name, config in custom_configs.items():
        print(f"{name}: {config.__dict__}")
