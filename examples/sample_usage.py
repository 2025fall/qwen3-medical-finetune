#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3åŒ»å­¦é—®ç­”å¾®è°ƒé¡¹ç›®ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†
"""

import os
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def load_model_and_tokenizer(model_type="lora"):
    """
    åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_type (str): æ¨¡å‹ç±»å‹ï¼Œ"lora" æˆ– "full"
    
    Returns:
        tuple: (model, tokenizer)
    """
    if model_type == "lora":
        # LoRAå¾®è°ƒæ¨¡å‹
        base_dir = "models/Qwen/Qwen3-1.7B"
        adapter_dir = "models/lora/final_lora"
        
        if not os.path.exists(base_dir) or not os.path.exists(adapter_dir):
            raise FileNotFoundError("LoRAæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬")
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_dir, 
            torch_dtype=torch.bfloat16, 
            device_map="auto", 
            trust_remote_code=True
        )
        model = PeftModel.from_pretrained(base_model, adapter_dir).eval()
        tokenizer = AutoTokenizer.from_pretrained(base_dir, use_fast=False, trust_remote_code=True)
        
    else:
        # å…¨å‚æ•°å¾®è°ƒæ¨¡å‹
        model_dir = "models/full/final_model"
        
        if not os.path.exists(model_dir):
            raise FileNotFoundError("å…¨å‚æ•°å¾®è°ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_dir, 
            torch_dtype=torch.bfloat16, 
            device_map="auto", 
            trust_remote_code=True
        ).eval()
        tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)
    
    return model, tokenizer

def generate_response(model, tokenizer, question, system_prompt=None):
    """
    ç”Ÿæˆå›ç­”
    
    Args:
        model: è¯­è¨€æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        question (str): ç”¨æˆ·é—®é¢˜
        system_prompt (str): ç³»ç»Ÿæç¤ºè¯
    
    Returns:
        str: æ¨¡å‹å›ç­”
    """
    if system_prompt is None:
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºå¸¦æœ‰æ€è€ƒçš„å›ç­”ã€‚"
    
    # æ„å»ºå¯¹è¯æ¶ˆæ¯
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question}
    ]
    
    # åº”ç”¨èŠå¤©æ¨¡æ¿
    text = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer([text], return_tensors="pt").to("cuda")
    
    # ç”Ÿæˆå›ç­”
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            do_sample=True
        )
    
    # è§£ç è¾“å‡º
    response = tokenizer.decode(
        outputs[0][len(inputs.input_ids[0]):], 
        skip_special_tokens=True
    )
    
    return response

def parse_think_answer(response):
    """
    è§£ææ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆ
    
    Args:
        response (str): æ¨¡å‹å›ç­”
    
    Returns:
        tuple: (æ€è€ƒè¿‡ç¨‹, ç­”æ¡ˆ)
    """
    import re
    
    # æå–<think>æ ‡ç­¾å†…å®¹
    think_match = re.search(r"<think>(.*?)</think>", response, re.DOTALL)
    if think_match:
        think_content = think_match.group(1).strip()
        # ç§»é™¤<think>æ ‡ç­¾åçš„å†…å®¹ä½œä¸ºç­”æ¡ˆ
        answer_content = re.sub(r"<think>.*?</think>\s*", "", response, flags=re.DOTALL).strip()
    else:
        think_content = ""
        answer_content = response
    
    return think_content, answer_content

def batch_inference(model, tokenizer, questions, system_prompt=None):
    """
    æ‰¹é‡æ¨ç†
    
    Args:
        model: è¯­è¨€æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        questions (list): é—®é¢˜åˆ—è¡¨
        system_prompt (str): ç³»ç»Ÿæç¤ºè¯
    
    Returns:
        list: å›ç­”åˆ—è¡¨
    """
    results = []
    
    for question in questions:
        print(f"å¤„ç†é—®é¢˜: {question}")
        
        response = generate_response(model, tokenizer, question, system_prompt)
        think_content, answer_content = parse_think_answer(response)
        
        result = {
            "question": question,
            "think": think_content,
            "answer": answer_content,
            "full_response": response
        }
        
        results.append(result)
        print(f"å›ç­”: {answer_content}")
        print("-" * 50)
    
    return results

def save_results(results, output_file="inference_results.json"):
    """
    ä¿å­˜æ¨ç†ç»“æœ
    
    Args:
        results (list): æ¨ç†ç»“æœ
        output_file (str): è¾“å‡ºæ–‡ä»¶å
    """
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Qwen3åŒ»å­¦é—®ç­”å¾®è°ƒé¡¹ç›®ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    lora_exists = os.path.exists("models/lora/final_lora")
    full_exists = os.path.exists("models/full/final_model")
    
    if not lora_exists and not full_exists:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ï¼š")
        print("   python scripts/train_lora.py  # LoRAå¾®è°ƒ")
        print("   python scripts/train_full.py  # å…¨å‚æ•°å¾®è°ƒ")
        return
    
    # é€‰æ‹©æ¨¡å‹ç±»å‹
    model_type = "lora" if lora_exists else "full"
    print(f"ğŸ“¦ ä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}")
    
    try:
        # åŠ è½½æ¨¡å‹
        print("ğŸ”„ åŠ è½½æ¨¡å‹ä¸­...")
        model, tokenizer = load_model_and_tokenizer(model_type)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # ç¤ºä¾‹é—®é¢˜
        sample_questions = [
            "æˆ‘æœ€è¿‘æ€»æ˜¯å¤±çœ ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ",
            "ä¸¤å²å°å­©å‘çƒ­39.5â„ƒè¯¥å¦‚ä½•å¤„ç†ï¼Ÿ",
            "é¤åä¸Šè…¹ç—›ä¼´åé…¸åº”è¯¥æ³¨æ„ä»€ä¹ˆï¼Ÿ",
            "çªç„¶å‰§çƒˆèƒ¸ç—›å‡ºå†·æ±—ï¼Œè¿˜å‘¼å¸å›°éš¾ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ"
        ]
        
        print("\nğŸ” å¼€å§‹æ‰¹é‡æ¨ç†...")
        results = batch_inference(model, tokenizer, sample_questions)
        
        # ä¿å­˜ç»“æœ
        save_results(results)
        
        print("\nğŸ‰ æ¨ç†å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt")
        print("2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUå†…å­˜")
        print("3. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

if __name__ == "__main__":
    main()
